name_run: Hypertuning

max_val_bs: 32

news_encoder:
  cfg_cat:
    num_layers: 3
    hidden_size: 128
    output_size: 33 
  cfg_ner:
    num_layers: 3
    hidden_size: 64
    output_size: 3 
    extend_output_size: 7

user_encoder:
  hidden_size: 128

trainer:
  aux_scaler: 0.3
  optimizer: adam
  epochs: 10
  lr_user: 0.00010905 # Rounded to 5 decimal places
  lr_news: 1.3756e-06
  lr_bert: 5.2667e-06
  batch_size: 32
  skip_ner: False
  skip_cat: False
  skip_gs: False

dataset:
  data_dir: data_small
  history_size: 12
  max_title_length: 20
  npratio: 4
  dataset_fraction: 0.005
  extended_NER: False

model:
  hidden: 768
  pretrained_model_name: bert-base-multilingual-uncased

lora_config:
  r: 8  # Rank of the decomposition
  lora_alpha: 32  # Scaling factor
  target_modules:
    - "query"
    - "value"  # Targeted modules in BERT
  lora_dropout: 0.1  # Dropout rate
  bias: "none"  # Bias type 

hypertuning:
  hidden_size: [64, 128, 256, 512]
  num_layers:
    min: 1
    max: 3
  batch_size: [16,32]
  lr:
    min: 1e-06
    max: 5e-03
  optimizer: [adam, adamw, sgd]
  n_trials: 20

wandb: True