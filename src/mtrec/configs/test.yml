#test config
news_encoder:   
  tok_size : 16 #TODO link to bert instead of hardcode
  embedding_dim : 16
  num_classes : 3
  num_ner : 4

user_encoder:
  input_size : 16 #TODO link to bert instead of hardcode
  embedding_dim : 16



trainer:
  optimizer : adam
  lr_user : 0.001
  lr_news : 0.001
  batch_size: 32

dataset:
  data_dir: ../../data_demo
  history_size: 30
  max_title_length: 30
  #batch_size: 32
  npratio: 4
  dataset_fraction: 0.01

model:
  hidden: 768
  pretrained_model_name: bert-base-uncased