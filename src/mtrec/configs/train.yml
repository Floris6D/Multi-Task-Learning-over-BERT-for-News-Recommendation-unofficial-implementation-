#test config
news_encoder:   
  input_size : 768 #TODO link to bert instead of hardcode
  embedding_dim : 16
  num_classes : 25 #TODO link to dataset instead of hardcode
  num_ner : 3 

user_encoder:
  input_size : 768 #TODO link to bert instead of hardcode
  embedding_dim : 16

trainer:
  optimizer : adam
  epochs: 1000
  lr_user : 0.001
  lr_news : 0.001
  lr_bert : 0.0001
  batch_size: 128

dataset:
  data_dir: ../../data/ebnerd_large
  history_size: 20
  max_title_length: 30
  npratio: 4
  dataset_fraction: 1

model:
  hidden: 768
  pretrained_model_name: bert-base-multilingual-uncased